# -*- coding: utf-8 -*-
"""Copy of Pratica-CursoGNN-FakeNews.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lnjngd9zFDhxAJfrdj_bvsSv1Xb6IKVq

# Dataset
"""

# !gdown 
#  pip install -U sentence-transformers
#  pip install tf-kera
#  pip install torch-geometric
#  conda install -c conda-forge ffmpeg
#

import random
import numpy as np
import pandas as pd
import networkx as nx
import torch_geometric.utils as utils
import matplotlib.animation as animation
import plotly.io as pio
pio.renderers.default = 'browser'
from matplotlib.animation import PillowWriter
from torch_geometric.nn import GAE
from sentence_transformers import SentenceTransformer
from sklearn.neighbors import kneighbors_graph


#%%
#
#   carregar o dataset
#

df = pd.read_pickle('fcn.pkl')
df = df[['text','class']]
df

#%%
#
#  Geração de Embedding - transforma o texto em vetor
#
#  A chamada sentence_transformers pertence à biblioteca sentence-transformers, 
#  que é construída sobre o Hugging Face Transformers e otimizada para gerar embeddings 
#  de sentenças e comparação semântica de textos.
#
#

model = SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased')

embeddings = model.encode(df.text)

df.at[:,'embedding'] = list(embeddings)
df

#%%

# Gerando Grafo
#
# Construir um grafo de vizinhança onde cada nó representa uma amostra do DataFrame df, 
# conectada a seus 3 vizinhos mais próximos com base nos embeddings.
# A é a matriz de adjancência
# df.embedding.to_list() transforma a coluna de embeddings do DataFrame em uma lista de vetores.
# kneighbors_graph(..., 3) usa os 3 vizinhos mais próximos para cada vetor (ponto).
# A é uma matriz esparsa onde A[i, j] = 1 se o ponto i é vizinho do ponto j.



A = kneighbors_graph(df.embedding.to_list(), 3)

G = nx.Graph(A)

for node in G.nodes():
  G.nodes[node]['features'] = df.iloc[node]['embedding']
  G.nodes[node]['label'] = 0 if df.iloc[node]['class'] == -1 else 1
  G.nodes[node]['text'] = df.iloc[node]['text']

#%%

# Faz o gráfico do Grafo no Plotly

list(G.nodes)[:10]

list(G.edges)[:10]

G.nodes[0]['features'][:10]

G.nodes[0]['label']

from plotly import graph_objs as go

def show_graph(G):
  ### ARESTAS
  edge_x = []
  edge_y = []

  # adicionando as coordenadas
  for edge in G.edges():
      x0, y0 = G.nodes[edge[0]]['pos']
      x1, y1 = G.nodes[edge[1]]['pos']
      edge_x.append(x0)
      edge_x.append(x1)
      edge_x.append(None)
      edge_y.append(y0)
      edge_y.append(y1)
      edge_y.append(None)

  # definindo cor e estilo das arestas
  edge_trace = go.Scatter(
      x=edge_x, y=edge_y,
      line=dict(width=2, color='#888'),
      hoverinfo='none',
      mode='lines')

  ### VÉRTICES
  node_x = []
  node_y = []

  # adicionando as coordenadas
  for node in G.nodes():
      x, y = G.nodes[node]['pos']
      node_x.append(x)
      node_y.append(y)

  # definindo cor e estilo dos vértices
  node_trace = go.Scatter(
      x=node_x, y=node_y,
      mode='markers',
      hoverinfo='text',
      marker=dict(
          size=10,
          line_width=2))


  node_labels = []
  texts = []
  for node in G.nodes():
    node_labels.append(G.nodes[node]['label'])
    texts.append(G.nodes[node]['text'][:100])

  node_trace.marker.color = node_labels
  node_trace.hovertext = texts

  # visualizando!
  fig = go.Figure(data=[edge_trace, node_trace],
              layout=go.Layout(
                  showlegend=False,
                  hovermode='closest',
                  margin=dict(b=20,l=5,r=5,t=40),
                  xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
                  yaxis=dict(showgrid=False, zeroline=False, showticklabels=False))
                  )
  fig.show()

pos = nx.spring_layout(G,seed=42) # obtém coordenadas dos vértices para visualização
for node in G.nodes():
  G.nodes[node]['pos'] = pos[node]

show_graph(G)

#%%
"""# Graph Autoencoder
Por que usar um Autoencoder para Grafos (Graph Autoencoder - GAE)?
Motivação:
Dados em forma de grafo (como redes sociais, citações, moléculas, etc.) têm estrutura não-euclidiana.
Um autoencoder é uma arquitetura que aprende a compactar e reconstruir dados. Quando aplicado a grafos, o objetivo é:
Aprender uma representação vetorial (embedding) de cada nó, que preserve a estrutura do grafo.
Com isso, você pode:

Fazer redução de dimensionalidade

Prever conexões (link prediction)

Classificar nós com poucos rótulos (semi-supervisionado)

Visualizar padrões na estrutura do grafo

"""

# !pip install torch-geometric


from torch_geometric.nn import GCNConv
import torch
import torch_geometric.utils as utils

class GCNEncoder(torch.nn.Module):
  # Cria um encoder com duas camadas de convolução sobre grafos (GCNConv, do PyTorch Geometric).
  # Transforma os features de entrada (H0) em uma representação latente (H2), passando por ReLU.
  def __init__(self, in_channels, out_channels):
    super().__init__()
    self.conv1 = GCNConv(in_channels, 2 * out_channels)
    self.conv2 = GCNConv(2 * out_channels, out_channels)

  def forward(self, H0, A):
    # H0: matriz de features dos nós.
    # A: edge_index (a lista das conexões do grafo no formato do PyTorch Geometric).
    # Resultado: vetor latente (embedding) de c
    H1 = self.conv1(H0, A).relu()
    H2 = self.conv2(H1, A)
    return H2

def train(gae, optimizer, graph):
   # Treina o Graph Autoencoder (gae) por um passo.
   # gae.encode(...): produz os embeddings dos nós.
   # gae.recon_loss(...): calcula a perda de reconstrução — ou seja, o quanto a rede consegue reconstruir as conexões reais entre nós a partir dos embeddings.
   # loss.backward() e optimizer.step(): retropropagação e otimização.
   # Retorna:
       # loss: o valor da perda (indicador de performance)
       # Hl: os embeddings dos nós

  gae.train()
  optimizer.zero_grad()

  Hl = gae.encode(graph.features.float(), graph.edge_index)

  loss = gae.recon_loss(Hl, graph.edge_index)

  loss.backward()
  optimizer.step()

  return float(loss), Hl

#%%
#
#
## Exexutando treinando da GNN


random.seed(81)
np.random.seed(81)
torch.manual_seed(81)
torch.cuda.manual_seed(81)

#
#   como saber se seu micro tem suporte ao cuda?
#   import torch
#   print(torch.cuda.is_available())
#
#   se for False, então
#   device = torch.device('cpu')

device = torch.device('cpu')

dataset = utils.from_networkx(G)

in_channels, out_channels = len(dataset.features[0]), 2

gae = GAE(GCNEncoder(in_channels, out_channels))

gae = gae.to(device)
gae = gae.float()

dataset = dataset.to(device)

optimizer = torch.optim.Adam(gae.parameters(), lr = 0.01)

epochs = 100

losses = []
embs_list = []

for epoch in range(epochs):
  loss, Hl = train(gae, optimizer, dataset)
  losses.append(loss)
  embs_list.append(Hl)

import matplotlib.pyplot as plt

plt.plot(losses)
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.show()

from IPython.display import HTML
from matplotlib import animation

labels_plot = np.array([G.nodes[node]['label'] for node in G.nodes])

#%%
#
#   versão com gif
#

from matplotlib import animation
from matplotlib.animation import PillowWriter
import matplotlib.pyplot as plt

def animate(i):
    embed = embs_list[i].detach().cpu().numpy()
    ax.clear()
    ax.scatter(embed[:, 0], embed[:, 1], s=50, c=labels_plot, cmap="hsv", vmin=-2, vmax=3)
    ax.set_title(f'Epoch {i} | Loss: {losses[i]:.2f}', fontsize=15, pad=30)
    ax.axis('off')

# Criação da figura
fig = plt.figure(figsize=(6, 6))
ax = fig.add_subplot()

# Garante que os ticks e eixos fiquem invisíveis
plt.axis('off')
plt.tick_params(left=False, bottom=False, labelleft=False, labelbottom=False)

# Animação
anim = animation.FuncAnimation(fig, animate, frames=np.arange(0, epochs, 1), interval=800, repeat=True)

# Salvar como GIF com PillowWriter
anim.save("animacao.gif", writer=PillowWriter(fps=1))

# Exibir no notebook
from IPython.display import Image
Image(filename="animacao.gif")

#%% 
#
## Obtendo representações da GNN"""
#

for node in G.nodes():
  G.nodes[node]['features_gae'] = Hl[node].detach().cpu().numpy()

G.nodes[0]['features_gae']

#%%
#
#
#  Separando nossas noticias para avaliar
#

x_int = []
x_out = []

for node in G.nodes():
  if G.nodes[node]['label'] == 1:
    x_int.append(G.nodes[node]['features_gae'])
  else:
    x_out.append(G.nodes[node]['features_gae'])

from sklearn.model_selection import train_test_split

x_train, x_test = train_test_split(x_int, test_size=0.1, random_state=81) # separando em treino e teste

#%% 
#
#
#  One-Class Learning

from sklearn.metrics import classification_report
from sklearn.svm import OneClassSVM as OCSVM

ocsvm = OCSVM(kernel='rbf', nu=0.2) # criando algoritmo

ocsvm.fit(x_train) # treinando algoritmo

y_int_pred = ocsvm.predict(x_test) # predizendo novos exemplos
y_out_pred = ocsvm.predict(x_out) # predizendo novos exemplos

y_true = np.concatenate([[1] * len(x_test), [-1] * len(x_out)])
y_pred = np.concatenate([y_int_pred, y_out_pred])

print(classification_report(y_true, y_pred)) # avaliando

#%%
# Vizualização

fig = plt.figure(figsize=(10, 8))

plt.scatter(np.array(x_train)[:, 0], np.array(x_train)[:, 1], c='gray', s=70, edgecolors='k') #treino

plt.scatter(np.array(x_out)[:, 0], np.array(x_out)[:, 1], c='blue', s=70, edgecolors='k')

plt.scatter(np.array(x_test)[:, 0], np.array(x_test)[:, 1], c='orange', s=70, edgecolors='k') #teste interesse

xx, yy = np.meshgrid(np.linspace(-3, 3, 500), np.linspace(-3, 3, 500))

Z = ocsvm.decision_function(np.c_[xx.ravel(), yy.ravel()])

Z = Z.reshape(xx.shape)

plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors='red') #esfera

plt.xlim((-0.5, 1.5))
plt.ylim((-1.5, 2.5))
plt.show()

#%%

# Modelage em grafo com embeddings na posição

for node in G.nodes():
  G.nodes[node]['pos'] = G.nodes[node]['features_gae']

show_graph(G)

#%%
#
#
#  GNN de duas classes?
#

from sklearn.model_selection import train_test_split

df_train, df_val = train_test_split(df, test_size = 0.1, random_state = 81, stratify=df['class'])

for i in df_train.index:
  G.nodes[i]['train'] = 1
  G.nodes[i]['val'] = 0

for i in df_val.index:
  G.nodes[i]['train'] = 0
  G.nodes[i]['val'] = 1

!pip install torcheval

from torch_geometric.nn import GCNConv
import torch
import torch.nn as nn
from torcheval.metrics.functional import multiclass_f1_score
import torch_geometric.utils as utils
from sklearn.metrics import classification_report

class GCN(torch.nn.Module):
  def __init__(self, in_channels, out_channels):
    super().__init__()
    self.conv1 = GCNConv(in_channels, 2 * out_channels)
    self.conv2 = GCNConv(2 * out_channels, out_channels)
    self.classification = nn.Linear(out_channels, 2)

  def forward(self, H0, A):
    H1 = self.conv1(H0, A).relu()
    H2 = self.conv2(H1, A).relu()

    return self.classification(H2), H2

def train_model(gcn, optmizer, graph, criterion):

  gcn.train()
  optmizer.zero_grad()

  predictions, Hl = gcn(graph.features, graph.edge_index)

  loss = criterion(predictions[mask], graph.label[mask])

  loss.backward()
  optmizer.step()

  return float(loss), Hl

def eval_node_classifier(gcn, mask, graph):
    gcn.eval()
    scores, _ = gcn(graph.features, graph.edge_index)
    pred = scores.argmax(dim=1)
    f1_macro = multiclass_f1_score(pred[mask], graph.label[mask], num_classes=2, average="macro")
    return f1_macro, nn.Softmax(dim=1)(scores), pred

random.seed(81)
np.random.seed(81)
torch.manual_seed(81)
torch.cuda.manual_seed(81)

criterion = nn.CrossEntropyLoss()
#
#  Se cuda estiver instalado... trocar cpu popr cuda
#
device = torch.device('cpu')
graph = utils.from_networkx(G)
in_channels, out_channels = len(graph.features[0]), 2
gcn = GCN(in_channels, out_channels)
gcn = gcn.to(device)
gcn = gcn.float()
graph = graph.to(device)
optmizer = torch.optim.Adam(gcn.parameters(), lr = 0.001)
epochs = 2000
losses = []
embs_list = []

for epoch in range(epochs):
  mask = torch.gt(graph.train, 0)
  loss, Hl = train_model(gcn, optmizer, graph, criterion)
  losses.append(loss)
  embs_list.append(Hl)
  if epoch % 10 == 0 or epoch == epochs - 1:
     f1, scores, y_pred = eval_node_classifier(gcn, torch.gt(graph.val, 0), graph)
     print(f'Epoch: {epoch:03d}, Train Loss: {loss:.3f}, Val F1: {f1:.3f}')
print(classification_report(y_pred[torch.gt(graph.val, 0 )].detach().cpu().numpy(), graph.label[torch.gt(graph.val, 0 )].detach().cpu().numpy()))

plt.plot(losses)
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.show()

for node in G.nodes():
  G.nodes[node]['pos'] = Hl[node].detach().cpu().numpy()

show_graph(G)
